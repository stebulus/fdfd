#!@bash@/bin/bash
set -euo pipefail
me=$(@coreutils@/bin/basename "${BASH_SOURCE[0]}")
here=$(@coreutils@/bin/dirname "${BASH_SOURCE[0]}")

XSD_DATE_FMT="%FT%T.%N%:z"
REDDIT_USER_AGENT="linux:org.amotlpaa.thing:v0.2.0 (by /u/stebulus)"

cleanup() { if [ -v tmp ]; then @coreutils@/bin/rm -rf "$tmp"; fi }
trap cleanup EXIT
tmp=$(@coreutils@/bin/mktemp --directory --tmpdir "$me".XXXXXXXX)

command_explorable() {
    if [ $# -ne 1 ]; then exit_usage "explorable DBDIR"; fi
    command_query "$1" '
        SELECT ?url
        WHERE {
            ?url amfd:review/amfd:worth-posting true .
            FILTER NOT EXISTS { ?explore amfd:explore-url ?url }
        }
        ' |
    @jq@/bin/jq -r '.results.bindings | .[] | .url.value'
}

command_explore() {
    local url exploredir now extracted wgetexit s3base s3dir workdir
    if [ $# -lt 1 ]; then exit_usage "explore S3BASE [URL ...]"; fi
    s3base=$1
    shift
    workdir=$(subtmpdir)
    for url in "$@"; do
        @coreutils@/bin/rm -rf "$workdir"/explore
        @coreutils@/bin/mkdir "$workdir"/explore
        s3dir=$(s3_stash_dir "$url")
        preamble_turtle
        echo "@base <$s3base/$s3dir/> ."
        echo "<.> rdf:type amfd:explore ;"
        echo "  amfd:explore-url <$url> ;"
        now=$(timestamp)
        echo "  amfd:when-start $now ."
        set +e
        @wget@/bin/wget \
            --output-file="$workdir"/explore/wget.log \
            --force-directories \
            --directory-prefix="$workdir"/explore \
            --adjust-extension \
            --recursive \
            --level=1 \
            --span-hosts \
            "$url"
        wgetexit=$?
        set -e
        case "$wgetexit" in
            0)  @findutils@/bin/find \
                    "$workdir"/explore \
                    -type f \
                    -iregex '.*\.html?$' \
                    -print0 |
                @findutils@/bin/xargs -0 "$here"/extract-feed |
                rdfuri |
                while read uri; do echo "$uri a amfd:feed ."; done
                ;;
            *)  echo "<.> amfd:error \"wget-${wgetexit}\" ."
                ;;
        esac
        stash "$workdir"/explore "$s3base/$s3dir" >&2
        now=$(timestamp)
        echo "<.> amfd:when-end $now ."
    done
}

command_feed() {
    if [ $# -gt 0 ]; then preamble_turtle; fi
    for url in "$@"; do echo "$url"; done |
    rdfuri |
    while read uri; do echo "$uri a amfd:feed ."; done
}

command_fetch() {
    local url fetchdir now extracted status s3base s3dir s3ff workdir
    if [ $# -lt 1 ]; then exit_usage "fetch S3BASE [URL ...]"; fi
    s3base=$1
    shift
    workdir=$(subtmpdir)
    for url in "$@"; do
        @coreutils@/bin/rm -rf "$workdir"/fetch
        @coreutils@/bin/mkdir "$workdir"/fetch
        s3dir=$(s3_stash_dir "$url")
        preamble_turtle
        echo "@base <$s3base/$s3dir/> ."
        echo "<.> rdf:type amfd:fetch ;"
        echo "  amfd:fetch-url <$url> ;"
        now=$(timestamp)
        echo "  amfd:when-start $now ."
        fetch_one "$url" "$workdir"/fetch
        status=$(curl_headers_status "$workdir"/fetch/headers)
        case "$status" in
            2*) extracted=none
                for type in rss atom reddit; do
                    if "$here/extract-$type" "$workdir"/fetch/entity 2>/dev/null; then
                        extracted=some
                    fi
                done
                case "$extracted" in
                    some) ;;
                    none) echo "<.> amfd:error \"extract\" ." ;;
                    *) echo "$me: internal error: extracted = $extracted" >&2
                       ;;
                esac
                ;;
            *)  echo "<.> amfd:error \"curl-${status:0:1}xx\" ."
                ;;
        esac
        stash "$workdir"/fetch "$s3base/$s3dir" >&2
        now=$(timestamp)
        echo "<.> amfd:when-end $now ."
    done
}

command_fetchable() {
    if [ $# -ne 1 ]; then exit_usage "fetchable DBDIR"; fi
    local fence
    feedfence=$(@coreutils@/bin/date +"${XSD_DATE_FMT}" --date "29 hours ago")
    redditfence=$(@coreutils@/bin/date +"${XSD_DATE_FMT}" --date "7 hours ago")
    longredditfence=$(@coreutils@/bin/date +"${XSD_DATE_FMT}" --date "3 days ago")
    command_query "$1" "
        SELECT ?url
        WHERE { {
            # plain old feeds that haven't been fetched recently
            ?url a amfd:feed .
            FILTER NOT EXISTS {
                ?url ^amfd:fetch-url/amfd:when-end ?time .
                FILTER (?time > \"$feedfence\"^^xsd:dateTime)
            }
            FILTER NOT EXISTS { ?url amfd:nope ?nope . }
        } UNION {
            # subreddits that have produced no items
            # since before longredditfence:
            # start over, just fetch current page of listings
            ?subreddit a amfd:subreddit ;
                amfdr:subreddit-name ?subredditname .
            FILTER NOT EXISTS {
                ?item a amfd:item ;
                    amfdr:subreddit ?subreddit ;
                    dc:source/amfd:when-end ?time .
                FILTER (?time > \"$longredditfence\"^^xsd:dateTime)
            }
            FILTER NOT EXISTS { ?subreddit amfd:nope ?nope . }
            BIND (URI(CONCAT(
                \"https://oauth.reddit.com/r/\",
                ?subredditname,
                \"/new?raw_json=1\"
            )) AS ?url)
        } UNION {
            # subreddits whose natural next page
            # either hasn't been fetched ever
            # or was fetched only before redditfence and was empty then
            {
                SELECT ?subreddit (MAX(?name) AS ?maxname)
                WHERE {
                    ?item amfdr:item-name ?name ;
                        amfdr:subreddit ?subreddit ;
                        dc:source/amfd:when-end ?time .
                    FILTER (?time > \"$longredditfence\"^^xsd:dateTime)
                    FILTER NOT EXISTS { ?subreddit amfd:nope ?nope . }
                }
                GROUP BY ?subreddit
                HAVING (bound(?maxname))
            }
            ?subreddit amfdr:subreddit-name ?subredditname .
            BIND (URI(CONCAT(
                \"https://oauth.reddit.com/r/\",
                ?subredditname,
                \"/new?raw_json=1&before=\",
                ?maxname
            )) AS ?url)
            FILTER NOT EXISTS {
                ?fetch amfd:fetch-url ?url .
                {
                    ?fetch amfd:when-end ?time .
                    FILTER (?time > \"$redditfence\"^^xsd:dateTime)
                } UNION {
                    FILTER EXISTS { ?item dc:source ?fetch }
                }
            }
        } }" |
    @jq@/bin/jq -r '.results.bindings | .[] | .url.value'
}

command_merge() {
    if [ $# -lt 1 ]; then exit_usage "merge DBDIR [FILE ...]"; fi
    local dbdir
    dbdir=$1
    s3base=$(@coreutils@/bin/cat "$dbdir"/s3base)
    shift
    if [ $# -eq 0 ]; then command_merge "$dbdir" -; fi
    for f in "$@"; do
        case "$f" in
            -) fullf="<stdin>" ;;
            *) fullf=$(@coreutils@/bin/readlink -e "$f") ;;
        esac
        g=$(nonce 20)
        {
            echo "# $fullf"
            @coreutils@/bin/date +"# ${XSD_DATE_FMT}"
            @jena@/bin/turtle "$f"
        } >"$dbdir/ff/.new.$g"
        @coreutils@/bin/mv "$dbdir/ff/.new.$g" "$dbdir/ff/$g"
        s3ff=$(s3_facts_file)
        /usr/bin/aws s3 cp "$dbdir/ff/$g" "$s3base/$s3ff"
    done
}

command_nope() {
    local now
    if [ $# -ge 1 ]; then preamble_turtle; fi
    for url in "$@"; do echo "$url"; done |
    rdfuri |
    while read uri; do
        now=$(timestamp)
        echo "$uri amfd:nope [ amfd:when $now ] ."
    done
}

command_query() {
    if [ $# -ne 2 ]; then exit_usage "query DBDIR QUERY"; fi
    local dbdir query work
    dbdir=$1
    query=$2
    work=$(subtmpdir)
    @coreutils@/bin/cat "$dbdir"/ff/* >"$work"/all-facts.nt
    {
        preamble_sparql
        echo "$query"
    } >"$work"/query.rq
    @jena@/bin/sparql \
        --results json \
        --data "$work"/all-facts.nt \
        --query "$work"/query.rq
}

command_reddit_auth() {
    local credsfile client_id client_secret redirect_uri
    case $# in
        1) credsfile=$1
           client_id=$(json_get .client_id "$credsfile")
           redirect_uri=$(json_get .redirect_uri "$credsfile")
           echo "https://www.reddit.com/api/v1/authorize?client_id=${client_id}&response_type=code&state=huzzah&redirect_uri=${redirect_uri}&duration=permanent&scope=read"
           ;;
        2) credsfile=$1
           code=$2
           client_id=$(json_get .client_id "$credsfile")
           client_secret=$(json_get .client_secret "$credsfile")
           redirect_uri=$(json_get .redirect_uri "$credsfile")
           @curl@/bin/curl --silent --show-error --fail \
               -XPOST \
               --data grant_type=authorization_code \
               --data code="$code" \
               --data redirect_uri="$redirect_uri" \
               --user "$client_id:$client_secret" \
               --user-agent "$REDDIT_USER_AGENT" \
               https://www.reddit.com/api/v1/access_token
           ;;
        *) exit_usage "reddit-auth CREDSFILE [CODE]"
           ;;
    esac
}

command_reddit_reauth() {
    if [ $# -ne 2 ]; then
        exit_usage "reddit-reauth CREDSFILE AUTHFILE"
    fi
    local authfile credsfile refresh_token suffix
    credsfile=$1
    authfile=$2
    client_id=$(json_get .client_id "$credsfile")
    client_secret=$(json_get .client_secret "$credsfile")
    redirect_uri=$(json_get .redirect_uri "$credsfile")
    refresh_token=$(json_get .refresh_token "$authfile")
    suffix=$(nonce 5)
    if @curl@/bin/curl --silent --show-error --fail \
        -XPOST \
        --data client_id="$client_id" \
        --data client_secret="$client_secret" \
        --data grant_type=refresh_token \
        --data refresh_token="$refresh_token" \
        --data scope=read \
        --data state=huzzah \
        --data duration=temporary \
        --data redirect_uri="$redirect_uri" \
        --user "$client_id:$client_secret" \
        --user-agent "$REDDIT_USER_AGENT" \
        https://www.reddit.com/api/v1/access_token \
        > "$tmp/$suffix"
    then
        @jq@/bin/jq \
            --arg refresh "$refresh_token" \
            'to_entries
              + [{"key": "refresh_token", "value": $refresh}]
              | from_entries' \
            "$tmp/$suffix" \
            > "${authfile}.new.${suffix}"
        @coreutils@/bin/mv "${authfile}.new.${suffix}" "${authfile}"
        json_get .expires_in "$authfile"
    else
        echo "$me: reddit reauthorization failed" >&2
        @coreutils@/bin/rm -f "${authfile}.new.${suffix}"
        return 1
    fi
}

command_review() {
    if [ $# -ne 0 ]; then exit_usage "review  # pipe from reviewable"; fi
    now=$(timestamp)
    preamble_turtle
    echo
    @jq@/bin/jq --arg now "$now" -r '
        .results.bindings | .[] | [
            @json "# Title: \(.title.value)",
            "# Source: \(.sourceurl.value)",
            "# Date: \(.date.value)",
            "<\(.link.value)>",
            "  amfd:review [",
            "    amfd:worth-posting ? ;",
            "    amfd:when \($now)",
            "  ] .",
            ""
        ] | .[]
        '
}

command_subreddit() {
    if [ $# -gt 0 ]; then preamble_turtle; fi
    for r in "$@"; do echo "$r"; done |
    @awk@/bin/awk --file "$here"/../lib/rdf.awk \
        --source '{
            name = rdfstr($0)
            uri = rdfuri("https://www.reddit.com/r/" $0)
            print uri " a amfd:subreddit ; "
            print "    amfdr:subreddit-name " name
        }'
}

command_postable() {
    if [ $# -ne 1 ]; then exit_usage "postable DBDIR"; fi
    command_query "$1" '
        SELECT ?link ?title ?sourceurl ?date
        WHERE {
            ?item a amfd:item ; amfd:link ?link .
            ?link amfd:review/amfd:worth-posting true .
            OPTIONAL { ?item dc:title ?title }
            OPTIONAL { ?item dc:source/amfd:fetch-url ?sourceurl }
            OPTIONAL { ?item dc:date ?date }
            FILTER NOT EXISTS { ?link amfd:post ?post }
        }
        '
}

command_reviewable() {
    local dbdir dateclause
    case $# in
        1)  dbdir=$1
            dateclause="OPTIONAL { ?someitem dc:date ?date }"
            ;;
        2)  dbdir=$1
            if ! @grep@/bin/grep -qE "^[0-9]{4}-[0-9]{2}-[0-9]{2}$" <<< $2
            then
                exit_usage "reviewable DBDIR [YYYY-MM-DD]"
            fi
            dateclause="?someitem dc:date ?date . FILTER (substr(str(?date ), 1, 10) = \"$2\")"
            ;;
        *)  exit_usage "reviewable DBDIR [YYYY-MM-DD]"
            ;;
    esac
    command_query "$dbdir" "
        SELECT ?link ?title ?sourceurl ?date
        WHERE {
            {
                SELECT ?link ?someitem (SAMPLE(?fetch) AS ?somefetch)
                WHERE {
                    {
                        SELECT ?link (SAMPLE(?item) AS ?someitem)
                        WHERE {
                            ?item a amfd:item ; amfd:link ?link .
                            FILTER NOT EXISTS
                                { ?link amfd:review ?review }
                            FILTER NOT EXISTS
                                { ?item amfdr:self true }
                        }
                        GROUP BY ?link
                    }
                    ?someitem dc:source ?fetch .
                    FILTER NOT EXISTS
                        { ?fetch amfd:fetch-url/amfd:nope ?nope . }
                }
                GROUP BY ?link ?someitem
            }
            OPTIONAL { ?someitem dc:title ?title }
            OPTIONAL { ?somefetch amfd:fetch-url ?sourceurl }
            $dateclause
        }
        "
}

curl_headers_status() {
    @coreutils@/bin/cat "$@" |
    @coreutils@/bin/tr -d '\r' |
    @awk@/bin/awk '
        BEGIN { statusLine = 1 }
        statusLine {
            status = $2
            statusLine = 0
        }
        /^$/ { statusLine = 1 }
        END { print status }
        '
}

fetch_one() {
    local url fetchdir extra_curl_opts reddit_token
    if [ $# -ne 2 ]; then exit_usage "fetch_one URL DIR"; fi
    url=$1
    fetchdir=$2
    case "$url" in
        https://oauth.reddit.com/*)
            reddit_token=$(@jq@/bin/jq -r .access_token \
                "$REDDIT_AUTH_FILE")
            extra_curl_opts=(
                -H "Authorization: Bearer $reddit_token"
                -A "$REDDIT_USER_AGENT"
            )
            ;;
        *)  unset -v extra_curl_opts
    esac
    @curl@/bin/curl \
        --compressed \
        --dump-header "$fetchdir"/headers \
        --location \
        --output "$fetchdir"/entity \
        --show-error \
        --silent \
        ${extra_curl_opts:+"${extra_curl_opts[@]}"} \
        "$url"
}

json_get() {
    key=$1
    file=$2
    @jq@/bin/jq -r \
        --arg me "$me" \
        --arg key "$key" \
        --arg file "$file" \
        "$key"' // error("\($me): \($key) not found in \($file)")' \
        "$file"
}

new_data_subdir() {
    local chars d
    chars=$(nonce 20)
    @coreutils@/bin/mkdir -p "$chars"
    echo "$chars"
}

nonce() {
    @coreutils@/bin/dd if=/dev/urandom count="$1" bs=1 2>/dev/null |
    @coreutils@/bin/base32 |
    @coreutils@/bin/tr A-Z a-z
}

preamble_raw() {
    echo "amfd: <tag:amotlpaa.org,2016:fdfd/>"
    echo "amfdr: <tag:amotlpaa.org,2016:fdfd-reddit/>"
    echo "dc: <http://purl.org/dc/terms/>"
    echo "rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>"
    echo "xsd: <http://www.w3.org/2001/XMLSchema#>"
}

preamble_sparql() {
    preamble_raw | @sed@/bin/sed 's,^,PREFIX ,'
}

preamble_turtle() {
    preamble_raw | @sed@/bin/sed 's,.*,@prefix & .,'
}

rdfuri() {
    @awk@/bin/awk --file "$here"/../lib/rdf.awk \
        --source '{ print rdfuri($0) }' \
        -- "$@"
}

s3_facts_file() {
    local datepart chars
    if [ $# -ne 0 ]; then exit_usage s3_facts_file; fi
    datepart=$(@coreutils@/bin/date -u +%F |@coreutils@/bin/tr - /)
    chars=$(nonce 10)
    echo "facts/$datepart/$chars.ttl"
}

s3_stash_dir() {
    local url datepart domain chars
    if [ $# -ne 1 ]; then exit_usage "s3_stash_dir URL"; fi
    url=$1
    datepart=$(@coreutils@/bin/date -u +%F |@coreutils@/bin/tr - /)
    domain=$(@sed@/bin/sed \
        -e 's,^[^:]*://,,' \
        -e 's,/.*$,,' \
        <<< "$url")
    chars=$(nonce 10)
    echo "stash/$datepart/$domain/$chars"
}

stash() {
    if [ $# -ne 2 ]; then exit_usage "stash dir s3dir"; fi
    local dir s3dir
    dir=${1%/}
    s3dir=${2%/}
    if [ -e "$dir"/stashed ]; then
        echo "$me: already stashed: $dir" >&2
        return 1
    fi
    /usr/bin/aws s3 sync "$dir/" "$s3dir/"
    echo "$s3dir/" >"$dir"/stashed
}

subtmpdir() {
    local name
    name=$(nonce 5)
    @coreutils@/bin/mkdir "$tmp/$name"
    echo "$tmp/$name"
}

timestamp() {
    local now
    now=$(@coreutils@/bin/date +"${XSD_DATE_FMT}")
    echo "\"$now\"^^xsd:dateTime"
}

exit_usage() {
    local base
    base="usage: $me"
    case $# in
        0)  echo "$base COMMAND [ARG ...]" >&2
            echo "where COMMAND is one of" >&2
            declare -F |
            @grep@/bin/grep '^declare -f command_' |
            @sed@/bin/sed -e 's,^declare -f command_,    ,' \
                -e 's/_/-/g' |
            @coreutils@/bin/sort >&2
            ;;
        *)  echo "$base $@" >&2
            ;;
    esac
    exit 1
}

if [ $# -lt 1 ]; then exit_usage; fi
command=$(echo "$1" | tr - _)
shift
"command_$command" "$@"
